{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://www.uao.edu.co/wp-content/uploads/2024/12/uao-logo-2-04.webp\" width=15%>\n",
    "\n",
    "\n",
    "<h2>UNIVERSIDAD AUT√ìNOMA DE OCCIDENTE</strong></h2>\n",
    "<h3>03/06/2025 CALI - COLOMBIA</strong></h3>\n",
    "<h3><strong>MAESTRIA EN INTELIGENCIA ARTIFICIAL Y CIENCIA DE DATOS</strong></h3>\n",
    "<h3><strong>ETL (EXTRACT, TRANSFORM AND LOAD)</strong></h3>\n",
    "<h3><strong>EJERCICIO EN CLASE 2 </strong> TRANSFORMACIONES</h3>\n",
    "<h3><strong>Profesor:</strong> JAVIER ALEJANDRO VERGARA ZORRILLA</h3>\n",
    "<h3><strong>Alumno:</strong>><font color='lighblue'> 22500214 Yoniliman Galvis Aguirre </font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO ETL #2\n",
    "\n",
    "## Contexto\n",
    "Este conjunto de datos proviene de una empresa de comercio electr√≥nico, que contiene diversa informaci√≥n sobre los productos disponibles en la tienda.\n",
    "\n",
    "## Ejercicio\n",
    "*   Realiza todas las transformaciones necesarias para obtener un conjunto de datos limpio con las caracter√≠sticas requeridas para entrenar un modelo de aprendizaje autom√°tico que prediga si un producto es nuevo o usado.\n",
    "\n",
    "*   Una caracter√≠stica es una columna con informaci√≥n importante o relevante para resolver el problema.\n",
    "\n",
    "*   Toma en cuenta todas las consideraciones y supuestos que necesites. En la carpeta de Google Drive puedes encontrar el archivo data_clean.csv, el cual puedes usar como ejemplo de salida.\n",
    "\n",
    "*   Realiza todo el An√°lisis Exploratorio de Datos (EDA) que consideres necesario, utiliza gr√°ficos como apoyo y aplica todas las transformaciones requeridas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar Kernel\n",
    "Verificamos si el ambiente jupyter esta ejecutando el kernel en el entorno correcto, el resultado de las dos rutas debe coincidir, de lo contrario se debe de cambiar el kernel del jupyter notebook, una opcion es correr el enviroment desde poetry, en la terminal ejecute:\n",
    "```bash\n",
    "poetry run jupyter notebook\n",
    "```\n",
    "esto abrir√° una version web de jupyter, en otro caso cambie el kernel y use los venv disponibles\n",
    "\n",
    "Si el notebook esta ejecutando un kernel diferente a la carpeta del proyecto cuando instale librer√≠as se presentar√°n fallas en la ejecucion del c√≥digo del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El entorno virtual activo de poetry est√° en: /home/ygalvis/Documents/Study/ETL_Ejercicio2/.venv\n",
      "El entorno virtual activo del kernel en el notebook est√° en: /home/ygalvis/Documents/Study/ETL_Ejercicio2/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Ejecutar el comando de poetry desde Python\n",
    "result = subprocess.run(['poetry', 'env', 'info', '--path'], capture_output=True, text=True)\n",
    "\n",
    "# Obtener la ruta del entorno virtual\n",
    "env_path = result.stdout.strip()\n",
    "\n",
    "# Obtener la ruta del ejecutable de Python activo\n",
    "python_path = shutil.which(\"python\")\n",
    "\n",
    "print(f\"El entorno virtual activo de poetry est√° en: {env_path}\")\n",
    "print(f\"El entorno virtual activo del kernel en el notebook est√° en: {python_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar Datos desde archivo JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion va a Leer el archivo JSONL usando la librer√≠a jsonlines\n",
    "\n",
    "# üîπ Funci√≥n para verificar si un valor es vac√≠o\n",
    "def es_vacio(x):\n",
    "    \"\"\" Verifica si x es un diccionario vac√≠o, lista vac√≠a, array vac√≠o o NaN \"\"\"\n",
    "    return (\n",
    "        (isinstance(x, dict) and len(x) == 0) or \n",
    "        (isinstance(x, list) and len(x) == 0) or \n",
    "        (isinstance(x, np.ndarray) and x.size == 0) or \n",
    "        pd.isna(x)\n",
    "    )\n",
    "\n",
    "# üîπ Funci√≥n para leer el JSONL y limpiar el DataFrame\n",
    "def leer_jsonl(ruta_archivo, ruta_exportar):\n",
    "    \"\"\"\n",
    "    *   Leer un archivo JSONL y cargarlo en un DataFrame.\n",
    "    *   Eliminar columnas con valores nulos, listas vac√≠as ([]), o diccionarios vac√≠os ({}).\n",
    "    *   Exportar los encabezados y la primera fila a un archivo de texto.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo (str): Ruta del archivo JSONL.\n",
    "        ruta_exportar (str): Ruta del archivo de texto donde se exportar√°n los encabezados y la primera fila.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame limpio con las columnas vac√≠as eliminadas.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with jsonlines.open(ruta_archivo) as reader:\n",
    "            for obj in tqdm(reader, desc=\"Leyendo l√≠neas del archivo JSONL\"):\n",
    "                data.append(obj)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: No se encontr√≥ el archivo. Verifica la ruta.\")\n",
    "        return pd.DataFrame()\n",
    "    except jsonlines.InvalidLineError as e:\n",
    "        print(f\"‚ùå Error al leer una l√≠nea del archivo. Revisa el JSON: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inesperado: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è El archivo JSONL est√° vac√≠o o no se pudo cargar correctamente.\")\n",
    "        return df\n",
    "\n",
    "    # üîπ Identificar columnas a eliminar\n",
    "    columnas_a_eliminar = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Si hay listas, aplanamos con explode() para evaluar correctamente\n",
    "            df_temp = df[col].explode() if df[col].apply(lambda x: isinstance(x, list)).any() else df[col]\n",
    "\n",
    "            # Si todos los valores en la columna son vac√≠os seg√∫n `es_vacio`\n",
    "            if df_temp.apply(es_vacio).all():\n",
    "                columnas_a_eliminar.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error al procesar la columna {col}: {e}\")\n",
    "\n",
    "    # üîπ Eliminar las columnas vac√≠as\n",
    "    df.drop(columns=columnas_a_eliminar, inplace=True)\n",
    "    print(\"üóë Columnas eliminadas:\", columnas_a_eliminar)\n",
    "\n",
    "    # üîπ Exportar encabezados y primera fila alineados\n",
    "    with open(ruta_exportar, \"w\", encoding=\"utf-8\") as f:\n",
    "        columnas = df.columns.tolist()\n",
    "        valores = df.iloc[0].astype(str).tolist() if not df.empty else [\"\"] * len(columnas)\n",
    "        max_len = max(len(col) for col in columnas)\n",
    "\n",
    "        f.write(\"Encabezado y primer fila:\\n\\n\")\n",
    "        for col, val in zip(columnas, valores):\n",
    "            f.write(f\"{col.rjust(max_len)} : {val}\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Archivo exportado a: {ruta_exportar}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leyendo l√≠neas del archivo JSONL: 100000it [00:04, 20410.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóë Columnas eliminadas: ['coverage_areas', 'differential_pricing', 'subtitle']\n",
      "‚úÖ Archivo exportado a: Dataset/header_firstrow.txt\n",
      "                                                                                  0  \\\n",
      "seller_address                    {'comment': '', 'longitude': -58.3986709, 'id'...   \n",
      "warranty                                                                       None   \n",
      "sub_status                                                                       []   \n",
      "condition                                                                       new   \n",
      "seller_contact                                                                 None   \n",
      "deal_ids                                                                         []   \n",
      "base_price                                                                     80.0   \n",
      "shipping                          {'local_pick_up': True, 'methods': [], 'tags':...   \n",
      "non_mercado_pago_payment_methods  [{'description': 'Transferencia bancaria', 'id...   \n",
      "seller_id                                                                  74952096   \n",
      "variations                                                                       []   \n",
      "location                                                                         {}   \n",
      "site_id                                                                         MLA   \n",
      "listing_type_id                                                              bronze   \n",
      "price                                                                          80.0   \n",
      "attributes                                                                       []   \n",
      "buying_mode                                                              buy_it_now   \n",
      "tags                                                      [dragged_bids_and_visits]   \n",
      "listing_source                                                                        \n",
      "parent_item_id                                                         MLA568261029   \n",
      "category_id                                                               MLA126406   \n",
      "descriptions                                     [{'id': 'MLA578052519-912855983'}]   \n",
      "last_updated                                               2015-09-05T20:42:58.000Z   \n",
      "international_delivery_mode                                                    none   \n",
      "pictures                          [{'size': '500x375', 'secure_url': 'https://a2...   \n",
      "id                                                                     MLA578052519   \n",
      "official_store_id                                                               NaN   \n",
      "accepts_mercadopago                                                            True   \n",
      "original_price                                                                  NaN   \n",
      "currency_id                                                                     ARS   \n",
      "thumbnail                         http://mla-s1-p.mlstatic.com/5386-MLA435206787...   \n",
      "title                             Auriculares Samsung Originales Manos Libres Ca...   \n",
      "automatic_relist                                                              False   \n",
      "date_created                                               2015-09-05T20:42:53.000Z   \n",
      "secure_thumbnail                  https://a248.e.akamai.net/mla-s1-p.mlstatic.co...   \n",
      "stop_time                                                  2015-11-04T20:42:53.000Z   \n",
      "status                                                                       active   \n",
      "video_id                                                                       None   \n",
      "catalog_product_id                                                              NaN   \n",
      "initial_quantity                                                                  1   \n",
      "start_time                                                 2015-09-05T20:42:53.000Z   \n",
      "permalink                         http://articulo.mercadolibre.com.ar/MLA-578052...   \n",
      "geolocation                       {'latitude': -34.6280698, 'longitude': -58.398...   \n",
      "sold_quantity                                                                     0   \n",
      "available_quantity                                                                1   \n",
      "\n",
      "                                                                                  1  \n",
      "seller_address                    {'comment': '', 'longitude': -58.5059173, 'id'...  \n",
      "warranty                                                         NUESTRA REPUTACION  \n",
      "sub_status                                                                       []  \n",
      "condition                                                                      used  \n",
      "seller_contact                                                                 None  \n",
      "deal_ids                                                                         []  \n",
      "base_price                                                                   2650.0  \n",
      "shipping                          {'local_pick_up': True, 'methods': [], 'tags':...  \n",
      "non_mercado_pago_payment_methods  [{'description': 'Transferencia bancaria', 'id...  \n",
      "seller_id                                                                  42093335  \n",
      "variations                                                                       []  \n",
      "location                                                                         {}  \n",
      "site_id                                                                         MLA  \n",
      "listing_type_id                                                              silver  \n",
      "price                                                                        2650.0  \n",
      "attributes                                                                       []  \n",
      "buying_mode                                                              buy_it_now  \n",
      "tags                                                                             []  \n",
      "listing_source                                                                       \n",
      "parent_item_id                                                         MLA561574487  \n",
      "category_id                                                                MLA10267  \n",
      "descriptions                                     [{'id': 'MLA581565358-930764806'}]  \n",
      "last_updated                                               2015-09-26T18:08:34.000Z  \n",
      "international_delivery_mode                                                    none  \n",
      "pictures                          [{'size': '499x334', 'secure_url': 'https://a2...  \n",
      "id                                                                     MLA581565358  \n",
      "official_store_id                                                               NaN  \n",
      "accepts_mercadopago                                                            True  \n",
      "original_price                                                                  NaN  \n",
      "currency_id                                                                     ARS  \n",
      "thumbnail                         http://mla-s1-p.mlstatic.com/23223-MLA20245018...  \n",
      "title                             Cuchillo Daga Acero Carb√≥n Casco Yelmo Solinge...  \n",
      "automatic_relist                                                              False  \n",
      "date_created                                               2015-09-26T18:08:30.000Z  \n",
      "secure_thumbnail                  https://a248.e.akamai.net/mla-s1-p.mlstatic.co...  \n",
      "stop_time                                                  2015-11-25T18:08:30.000Z  \n",
      "status                                                                       active  \n",
      "video_id                                                                       None  \n",
      "catalog_product_id                                                              NaN  \n",
      "initial_quantity                                                                  1  \n",
      "start_time                                                 2015-09-26T18:08:30.000Z  \n",
      "permalink                         http://articulo.mercadolibre.com.ar/MLA-581565...  \n",
      "geolocation                       {'latitude': -34.5935524, 'longitude': -58.505...  \n",
      "sold_quantity                                                                     0  \n",
      "available_quantity                                                                1  \n"
     ]
    }
   ],
   "source": [
    "ruta_archivo = 'Dataset/MLA_100k.jsonlines'\n",
    "ruta_exportar = 'Dataset/header_firstrow.txt'\n",
    "df = leer_jsonl(ruta_archivo, ruta_exportar)\n",
    "\n",
    "# Mostrar todas las columnas en formato vertical con las dos primeras filas\n",
    "pd.options.display.max_rows = None\n",
    "print(df.head(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con estructuras anidadas: ['seller_address', 'sub_status', 'seller_contact', 'deal_ids', 'shipping', 'non_mercado_pago_payment_methods', 'variations', 'location', 'attributes', 'tags', 'descriptions', 'pictures', 'geolocation']\n"
     ]
    }
   ],
   "source": [
    "# üîπ Identificar columnas con diccionarios o listas (incluyendo vac√≠os)\n",
    "columnas_anidadas = []\n",
    "\n",
    "for col in df.columns:\n",
    "    # Ignorar columnas completamente vac√≠as\n",
    "    if df[col].dropna().empty:\n",
    "        continue\n",
    "    \n",
    "    # üîπ Verificar si la columna contiene diccionarios o listas\n",
    "    if df[col].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        columnas_anidadas.append(col)\n",
    "\n",
    "print(\"Columnas con estructuras anidadas:\", columnas_anidadas)\n",
    "df_anidadas = df[columnas_anidadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller_address</th>\n",
       "      <th>sub_status</th>\n",
       "      <th>seller_contact</th>\n",
       "      <th>deal_ids</th>\n",
       "      <th>shipping</th>\n",
       "      <th>non_mercado_pago_payment_methods</th>\n",
       "      <th>variations</th>\n",
       "      <th>location</th>\n",
       "      <th>attributes</th>\n",
       "      <th>tags</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>pictures</th>\n",
       "      <th>geolocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'comment': '', 'longitude': -58.3986709, 'id'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'local_pick_up': True, 'methods': [], 'tags':...</td>\n",
       "      <td>[{'description': 'Transferencia bancaria', 'id...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dragged_bids_and_visits]</td>\n",
       "      <td>[{'id': 'MLA578052519-912855983'}]</td>\n",
       "      <td>[{'size': '500x375', 'secure_url': 'https://a2...</td>\n",
       "      <td>{'latitude': -34.6280698, 'longitude': -58.398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'comment': '', 'longitude': -58.5059173, 'id'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'local_pick_up': True, 'methods': [], 'tags':...</td>\n",
       "      <td>[{'description': 'Transferencia bancaria', 'id...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 'MLA581565358-930764806'}]</td>\n",
       "      <td>[{'size': '499x334', 'secure_url': 'https://a2...</td>\n",
       "      <td>{'latitude': -34.5935524, 'longitude': -58.505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'comment': '', 'longitude': -58.4143948, 'id'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'local_pick_up': True, 'methods': [], 'tags':...</td>\n",
       "      <td>[{'description': 'Transferencia bancaria', 'id...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dragged_bids_and_visits]</td>\n",
       "      <td>[{'id': 'MLA578780872-916478256'}]</td>\n",
       "      <td>[{'size': '375x500', 'secure_url': 'https://a2...</td>\n",
       "      <td>{'latitude': -34.6233907, 'longitude': -58.414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'comment': '', 'longitude': -58.4929208, 'id'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'local_pick_up': True, 'methods': [], 'tags':...</td>\n",
       "      <td>[{'description': 'Transferencia bancaria', 'id...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 'MLA581877385-932309698'}]</td>\n",
       "      <td>[{'size': '441x423', 'secure_url': 'https://a2...</td>\n",
       "      <td>{'latitude': -34.6281894, 'longitude': -58.492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'comment': '', 'longitude': -58.5495042, 'id'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'local_pick_up': True, 'methods': [], 'tags':...</td>\n",
       "      <td>[{'description': 'Transferencia bancaria', 'id...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dragged_bids_and_visits]</td>\n",
       "      <td>[{'id': 'MLA576112692-902981678'}]</td>\n",
       "      <td>[{'size': '375x500', 'secure_url': 'https://a2...</td>\n",
       "      <td>{'latitude': -34.6346547, 'longitude': -58.549...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      seller_address sub_status  \\\n",
       "0  {'comment': '', 'longitude': -58.3986709, 'id'...         []   \n",
       "1  {'comment': '', 'longitude': -58.5059173, 'id'...         []   \n",
       "2  {'comment': '', 'longitude': -58.4143948, 'id'...         []   \n",
       "3  {'comment': '', 'longitude': -58.4929208, 'id'...         []   \n",
       "4  {'comment': '', 'longitude': -58.5495042, 'id'...         []   \n",
       "\n",
       "  seller_contact deal_ids                                           shipping  \\\n",
       "0           None       []  {'local_pick_up': True, 'methods': [], 'tags':...   \n",
       "1           None       []  {'local_pick_up': True, 'methods': [], 'tags':...   \n",
       "2           None       []  {'local_pick_up': True, 'methods': [], 'tags':...   \n",
       "3           None       []  {'local_pick_up': True, 'methods': [], 'tags':...   \n",
       "4           None       []  {'local_pick_up': True, 'methods': [], 'tags':...   \n",
       "\n",
       "                    non_mercado_pago_payment_methods variations location  \\\n",
       "0  [{'description': 'Transferencia bancaria', 'id...         []       {}   \n",
       "1  [{'description': 'Transferencia bancaria', 'id...         []       {}   \n",
       "2  [{'description': 'Transferencia bancaria', 'id...         []       {}   \n",
       "3  [{'description': 'Transferencia bancaria', 'id...         []       {}   \n",
       "4  [{'description': 'Transferencia bancaria', 'id...         []       {}   \n",
       "\n",
       "  attributes                       tags                        descriptions  \\\n",
       "0         []  [dragged_bids_and_visits]  [{'id': 'MLA578052519-912855983'}]   \n",
       "1         []                         []  [{'id': 'MLA581565358-930764806'}]   \n",
       "2         []  [dragged_bids_and_visits]  [{'id': 'MLA578780872-916478256'}]   \n",
       "3         []                         []  [{'id': 'MLA581877385-932309698'}]   \n",
       "4         []  [dragged_bids_and_visits]  [{'id': 'MLA576112692-902981678'}]   \n",
       "\n",
       "                                            pictures  \\\n",
       "0  [{'size': '500x375', 'secure_url': 'https://a2...   \n",
       "1  [{'size': '499x334', 'secure_url': 'https://a2...   \n",
       "2  [{'size': '375x500', 'secure_url': 'https://a2...   \n",
       "3  [{'size': '441x423', 'secure_url': 'https://a2...   \n",
       "4  [{'size': '375x500', 'secure_url': 'https://a2...   \n",
       "\n",
       "                                         geolocation  \n",
       "0  {'latitude': -34.6280698, 'longitude': -58.398...  \n",
       "1  {'latitude': -34.5935524, 'longitude': -58.505...  \n",
       "2  {'latitude': -34.6233907, 'longitude': -58.414...  \n",
       "3  {'latitude': -34.6281894, 'longitude': -58.492...  \n",
       "4  {'latitude': -34.6346547, 'longitude': -58.549...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anidadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leyendo l√≠neas del archivo JSONL: 100000it [00:05, 17259.07it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['tags'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m ruta_archivo = \u001b[33m'\u001b[39m\u001b[33mDataset/MLA_100k.jsonlines\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     74\u001b[39m ruta_exportar = \u001b[33m'\u001b[39m\u001b[33mDataset/header_firstrow.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m df_final = \u001b[43mleer_jsonl_y_limpiar_columnas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_archivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_exportar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Mostrar la primera fila transpuesta\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_final.head(\u001b[32m1\u001b[39m).T)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mleer_jsonl_y_limpiar_columnas\u001b[39m\u001b[34m(ruta_archivo, ruta_exportar)\u001b[39m\n\u001b[32m     56\u001b[39m     df_expanded_final = pd.concat(df_expanded_list, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Unir con el DataFrame original sin eliminar la columna 'id' original\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumnas_anidadas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_expanded_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Exportar los encabezados de las columnas y la primera fila a un archivo de texto\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ruta_exportar, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:10757\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[39m\n\u001b[32m  10747\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  10748\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m  10749\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10750\u001b[39m             other,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10755\u001b[39m             validate=validate,\n\u001b[32m  10756\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10758\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10764\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10765\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10767\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10768\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m  10769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:840\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    837\u001b[39m left = \u001b[38;5;28mself\u001b[39m.left[:]\n\u001b[32m    838\u001b[39m right = \u001b[38;5;28mself\u001b[39m.right[:]\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m llabels, rlabels = \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mright\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msuffixes\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[32m    845\u001b[39m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[32m    846\u001b[39m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[32m    848\u001b[39m     lmgr = left._mgr.reindex_indexer(\n\u001b[32m    849\u001b[39m         join_index,\n\u001b[32m    850\u001b[39m         left_indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    855\u001b[39m         use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    856\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:2721\u001b[39m, in \u001b[36m_items_overlap_with_suffix\u001b[39m\u001b[34m(left, right, suffixes)\u001b[39m\n\u001b[32m   2718\u001b[39m lsuffix, rsuffix = suffixes\n\u001b[32m   2720\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lsuffix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rsuffix:\n\u001b[32m-> \u001b[39m\u001b[32m2721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns overlap but no suffix specified: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_rename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2723\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrenamer\u001b[39m(x, suffix: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2724\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2725\u001b[39m \u001b[33;03m    Rename the left and right indices.\u001b[39;00m\n\u001b[32m   2726\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2737\u001b[39m \u001b[33;03m    x : renamed column name\u001b[39;00m\n\u001b[32m   2738\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: columns overlap but no suffix specified: Index(['tags'], dtype='object')"
     ]
    }
   ],
   "source": [
    "df_expanded_list = []  # Lista para almacenar DataFrames expandidos\n",
    "\n",
    "# Expandir cada columna anidada\n",
    "def leer_jsonl_y_limpiar_columnas(ruta_archivo, ruta_exportar):\n",
    "    \"\"\"\n",
    "    Lee un archivo JSONL, elimina las columnas vac√≠as y duplicadas, expande columnas anidadas,\n",
    "    y exporta los encabezados y la primera fila a un archivo de texto.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo (str): La ruta al archivo JSONL.\n",
    "        ruta_exportar (str): La ruta al archivo de texto donde se exportar√°n los encabezados y la primera fila.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame de pandas con las columnas vac√≠as, duplicadas eliminadas y columnas anidadas expandidas.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with jsonlines.open(ruta_archivo) as reader:\n",
    "            for obj in tqdm(reader, desc=\"Leyendo l√≠neas del archivo JSONL\"):\n",
    "                data.append(obj)\n",
    "    except FileNotFoundError:\n",
    "        print(\"El archivo no se encontr√≥. Verifica la ruta.\")\n",
    "        return pd.DataFrame()  # Retorna un DataFrame vac√≠o en caso de error\n",
    "    except jsonlines.InvalidLineError as e:\n",
    "        print(f\"Error al leer una l√≠nea del archivo: {e}\")\n",
    "        return pd.DataFrame()  # Retorna un DataFrame vac√≠o en caso de error\n",
    "\n",
    "    # Convertir la lista de diccionarios a un DataFrame de pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    # Eliminar columnas que contienen solo valores vac√≠os\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df = df[[col for col in df if not all(es_vacio(i) for i in df[col])]]\n",
    "\n",
    "    # Eliminar columnas duplicadas\n",
    "    #df = df.T.drop_duplicates().T\n",
    "\n",
    "    # Identificar columnas anidadas\n",
    "    columnas_anidadas = [col for col in df.columns if isinstance(df[col].iloc[0], dict)]\n",
    "\n",
    "    # Expandir cada columna anidada\n",
    "    df_expanded_list = []\n",
    "    for col in columnas_anidadas:\n",
    "        if col in df.columns:\n",
    "            df_expanded = pd.json_normalize(df[col], sep=\"_\")\n",
    "            \n",
    "            # Renombrar la columna 'id' si existe en df_expanded para evitar conflictos\n",
    "            if \"id\" in df_expanded.columns:\n",
    "                df_expanded.rename(columns={\"id\": f\"{col}_id\"}, inplace=True)\n",
    "\n",
    "            df_expanded_list.append(df_expanded)\n",
    "\n",
    "    # Concatenar los DataFrames expandidos\n",
    "    if df_expanded_list:\n",
    "        df_expanded_final = pd.concat(df_expanded_list, axis=1)\n",
    "        # Unir con el DataFrame original sin eliminar la columna 'id' original\n",
    "        df = df.drop(columns=columnas_anidadas).join(df_expanded_final)\n",
    "\n",
    "    # Exportar los encabezados de las columnas y la primera fila a un archivo de texto\n",
    "    with open(ruta_exportar, \"w\") as f:\n",
    "        columnas = df.columns.tolist()\n",
    "        valores = df.iloc[0].astype(str).tolist()\n",
    "        \n",
    "        max_len = max(len(col) for col in columnas)  # Longitud m√°xima de columna\n",
    "\n",
    "        for col, val in zip(columnas, valores):\n",
    "            f.write(\"{:>{width}} : {}\\n\".format(col, val, width=max_len))  # Alinear nombres\n",
    "\n",
    "    return df\n",
    "\n",
    "# Uso de la funci√≥n\n",
    "ruta_archivo = 'Dataset/MLA_100k.jsonlines'\n",
    "ruta_exportar = 'Dataset/header_firstrow.txt'\n",
    "df_final = leer_jsonl_y_limpiar_columnas(ruta_archivo, ruta_exportar)\n",
    "\n",
    "# Mostrar la primera fila transpuesta\n",
    "print(df_final.head(1).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m df_final = df_final[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_final \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(es_vacio(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df_final[col])]]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Eliminar columnas duplicadas\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_final = \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.T\n\u001b[32m      6\u001b[39m df_final.head(\u001b[32m1\u001b[39m).T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:6818\u001b[39m, in \u001b[36mDataFrame.drop_duplicates\u001b[39m\u001b[34m(self, subset, keep, inplace, ignore_index)\u001b[39m\n\u001b[32m   6815\u001b[39m inplace = validate_bool_kwarg(inplace, \u001b[33m\"\u001b[39m\u001b[33minplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6816\u001b[39m ignore_index = validate_bool_kwarg(ignore_index, \u001b[33m\"\u001b[39m\u001b[33mignore_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m6818\u001b[39m result = \u001b[38;5;28mself\u001b[39m[-\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m   6819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   6820\u001b[39m     result.index = default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:6958\u001b[39m, in \u001b[36mDataFrame.duplicated\u001b[39m\u001b[34m(self, subset, keep)\u001b[39m\n\u001b[32m   6956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6957\u001b[39m     vals = (col.values \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[32m-> \u001b[39m\u001b[32m6958\u001b[39m     labels, shape = \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   6960\u001b[39m     ids = get_group_index(labels, \u001b[38;5;28mtuple\u001b[39m(shape), sort=\u001b[38;5;28;01mFalse\u001b[39;00m, xnull=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   6961\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._constructor_sliced(duplicated(ids, keep), index=\u001b[38;5;28mself\u001b[39m.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:6926\u001b[39m, in \u001b[36mDataFrame.duplicated.<locals>.f\u001b[39m\u001b[34m(vals)\u001b[39m\n\u001b[32m   6925\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(vals) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m6926\u001b[39m     labels, shape = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m labels.astype(\u001b[33m\"\u001b[39m\u001b[33mi8\u001b[39m\u001b[33m\"\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:795\u001b[39m, in \u001b[36mfactorize\u001b[39m\u001b[34m(values, sort, use_na_sentinel, size_hint)\u001b[39m\n\u001b[32m    792\u001b[39m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[32m    793\u001b[39m             values = np.where(null_mask, na_value, values)\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     codes, uniques = \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m0\u001b[39m:\n\u001b[32m    802\u001b[39m     uniques, codes = safe_sort(\n\u001b[32m    803\u001b[39m         uniques,\n\u001b[32m    804\u001b[39m         codes,\n\u001b[32m   (...)\u001b[39m\u001b[32m    807\u001b[39m         verify=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    808\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/ETL_Ejercicio2/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:595\u001b[39m, in \u001b[36mfactorize_array\u001b[39m\u001b[34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[39m\n\u001b[32m    592\u001b[39m hash_klass, values = _get_hashtable_algo(values)\n\u001b[32m    594\u001b[39m table = hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m uniques, codes = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[32m    604\u001b[39m uniques = _reconstruct_data(uniques, original.dtype, original)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7281\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "# Eliminar columnas que contienen solo valores vac√≠os\n",
    "df_final = df_final.dropna(axis=1, how='all')\n",
    "df_final = df_final[[col for col in df_final if not all(es_vacio(i) for i in df_final[col])]]\n",
    "# Eliminar columnas duplicadas\n",
    "df_final = df_final.T.drop_duplicates().T\n",
    "df_final.head(1).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
